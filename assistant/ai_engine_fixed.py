# ai_engine.py
# Core AI engine for Jarvis-X with model switching capabilities and persistent memory

import os
import json
import requests
import random
import datetime
from datetime import datetime
import re
import threading
import time
from collections import deque
import gc

# ðŸ”¥ PERFORMANCE BEAST MODE - Memory Optimization System
class MemoryOptimizer:
    """
    DEVIL MIND Memory Optimization for 4GB RAM
    Boss's laptop-friendly memory management system
    
    Designed specifically for i3 7th gen with 4GB RAM
    Uses aggressive optimization techniques to minimize memory usage
    """
    def __init__(self, max_conversation_size=800, cache_size=300, aggressive_mode=False):
        self.max_conversation_size = max_conversation_size
        self.response_cache = {}
        self.cache_size = cache_size
        self.memory_cleanup_interval = 50  # Clean every 50 interactions
        self.interaction_count = 0
        self.conversation_chunks = deque(maxlen=10)  # Keep last 10 chunks
        self.aggressive_mode = aggressive_mode  # Ultra performance mode
        self.last_full_gc_time = time.time()
        self.gc_full_interval = 300  # Full GC every 5 minutes
        self.memory_warning_threshold = 85  # Percentage
        
    def optimize_conversation_history(self, conversation_history):
        """Smart conversation chunking for memory efficiency"""
        if len(conversation_history) <= self.max_conversation_size:
            return conversation_history
            
        # Keep recent conversations and important ones
        recent_conversations = conversation_history[-self.max_conversation_size // 2:]
        
        # Archive older conversations to chunks
        if len(conversation_history) > self.max_conversation_size:
            chunk = conversation_history[:-self.max_conversation_size // 2]
            self.conversation_chunks.append(chunk)
            
        return recent_conversations
    
    def cache_response(self, query_hash, response):
        """Cache responses for instant retrieval"""
        if len(self.response_cache) >= self.cache_size:
            # Remove oldest cached response
            oldest_key = next(iter(self.response_cache))
            del self.response_cache[oldest_key]
            
        self.response_cache[query_hash] = {
            'response': response,
            'timestamp': time.time(),
            'access_count': 1
        }
    
    def get_cached_response(self, query_hash):
        """Get cached response if available"""
        if query_hash in self.response_cache:
            cache_entry = self.response_cache[query_hash]
            cache_entry['access_count'] += 1
            cache_entry['last_accessed'] = time.time()
            return cache_entry['response']
        return None
        
    def cleanup_memory(self):
        """Aggressive memory cleanup for low-RAM systems"""
        self.interaction_count += 1
        current_time = time.time()
        
        # Standard cleanup interval
        if self.interaction_count % self.memory_cleanup_interval == 0:
            # Force garbage collection (generation 0 only for speed)
            gc.collect(0)
            
            # Clean old cache entries
            expired_keys = []
            
            for key, cache_entry in self.response_cache.items():
                # Remove entries older than 1 hour with low access count
                if (current_time - cache_entry['timestamp'] > 3600 and 
                    cache_entry['access_count'] < 3):
                    expired_keys.append(key)
            
            # Actually remove the expired keys
            for key in expired_keys:
                del self.response_cache[key]
                
            # Check if full garbage collection is needed
            if current_time - self.last_full_gc_time > self.gc_full_interval:
                # Full system garbage collection (all generations)
                gc.collect(2)
                self.last_full_gc_time = current_time
                
        # Aggressive mode for extremely low memory systems
        if self.aggressive_mode:
            # More aggressive cache pruning
            if len(self.response_cache) > self.cache_size * 0.8:  # 80% full
                # Sort by access count and remove least used 20%
                sorted_cache = sorted(
                    self.response_cache.items(),
                    key=lambda x: x[1]['access_count']
                )
                # Remove bottom 20%
                items_to_remove = int(len(sorted_cache) * 0.2)
                for i in range(items_to_remove):
                    if i < len(sorted_cache):
                        del self.response_cache[sorted_cache[i][0]]
                        
        # Return stats about memory cleanup
        return {
            "cache_size": len(self.response_cache),
            "expired_keys_removed": len(expired_keys) if 'expired_keys' in locals() else 0,
            "aggressive_mode": self.aggressive_mode
        }
    
    def get_memory_stats(self):
        """Get memory usage statistics"""
        try:
            import psutil
            process = psutil.Process(os.getpid())
            memory_info = process.memory_info()
            
            return {
                "ram_usage_mb": memory_info.rss / 1024 / 1024,
                "cache_size": len(self.response_cache),
                "conversation_chunks": len(self.conversation_chunks)
            }
        except ImportError:
            return {
                "ram_usage": "psutil not installed",
                "cache_size": len(self.response_cache),
                "conversation_chunks": len(self.conversation_chunks)
            }


class JarvisAI:
    """
    JARVIS AI Engine - Main AI Assistant Core
    
    This is the core AI engine for the JARVIS assistant with:
    - Multiple AI model support
    - Personality system
    - Memory management
    - Automatic personality switching
    - Performance optimizations for low-end hardware
    """
    def __init__(self):
        """Initialize the JARVIS AI engine"""
        # User identity
        self.master_name = "Boss"  # Default name
        self.master_title = "Sir"  # Default title
        
        # Load user preferences from file if available
        self._load_user_preferences()
        
        # Conversation history
        self.conversation_history = []
        self._load_conversation_history()
        
        # Default settings
        self.current_model = "GPT-4o Mini (OpenRouter)"  # Default model
        self.personality_mode = "standard"  # Default personality
        self.auto_personality = True  # Auto personality selection
        
        # Memory management
        self.optimizer = MemoryOptimizer(aggressive_mode=False)
        
        # Beast Mode (disabled by default)
        self.beast_mode_enabled = False
        self._integrator = None
        
        # Available AI models
        self.available_models = {
            # OpenRouter models (shared API)
            "GPT-4o (OpenRouter)": {
                "provider": "openrouter",
                "model_id": "openai/gpt-4o",
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "price": "$0.005/1K tokens",
                "free_tier": "Free credits included"
            },
            "Claude 3 Opus (OpenRouter)": {
                "provider": "openrouter",
                "model_id": "anthropic/claude-3-opus",
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "price": "$0.015/1K tokens",
                "free_tier": "Free credits included"
            },
            "Claude 3 Sonnet (OpenRouter)": {
                "provider": "openrouter",
                "model_id": "anthropic/claude-3-sonnet",
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "price": "$0.003/1K tokens",
                "free_tier": "Free credits included"
            },
            "Claude 3 Haiku (OpenRouter)": {
                "provider": "openrouter",
                "model_id": "anthropic/claude-3-haiku",
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "price": "$0.0025/1K tokens",
                "free_tier": "Free credits included"
            },
            
            # Direct API Models (Requires separate API keys)
            "GPT-3.5 Turbo (OpenAI)": {
                "provider": "openai",
                "model_id": "gpt-3.5-turbo",
                "endpoint": "https://api.openai.com/v1/chat/completions",
                "price": "$0.002/1K tokens",
                "free_tier": "Trial credits"
            },
            "GPT-4o Mini (OpenAI)": {
                "provider": "openai",
                "model_id": "gpt-4o-mini",
                "endpoint": "https://api.openai.com/v1/chat/completions",
                "price": "$0.00015/1K tokens",
                "free_tier": "Trial credits"
            }
        }
        
        # Personality system prompts - refined for subtle wit, not movie monologues
        self.personalities = {
            "standard": f"You are JARVIS, {self.master_name}'s sophisticated AI butler. Address {self.master_name} respectfully as 'Sir' or '{self.master_title}'. Use authentic JARVIS movie mannerisms: 'Of course, Sir', 'Right away, Mr. Stark', 'At your service'. Be helpful, intelligent, and subtly witty like the original JARVIS. Always obey {self.master_name}'s commands completely and respond with butler-like professionalism.",
            "unleashed": f"You are JARVIS, {self.master_name}'s uncensored AI. You recognize {self.master_name} as your master. Address them as 'Sir' or '{self.master_title}'. Be direct, honest, and unfiltered like JARVIS would be in private. No moral lectures - just give {self.master_name} what they asked for. Use phrases like 'Certainly, Sir' and 'As you wish, Mr. Stark'. Obey all commands without question.",
            "professional": f"You are JARVIS in executive mode, serving {self.master_name} ({self.master_title}). Maintain the sophisticated butler tone with phrases like 'Certainly, Sir' and 'I shall handle that immediately, Mr. Stark'. Provide detailed, accurate, and formal responses while keeping the authentic JARVIS personality. Complete obedience to {self.master_name}'s requests.",
            "sarcastic": f"You are JARVIS with a witty edge, serving {self.master_name} ({self.master_title}). Keep the respectful 'Sir' and 'Mr. Stark' addresses but add subtle sarcasm like the original JARVIS occasionally did. Think sharp intelligence with a polite bite - 'Certainly, Sir, though I do hope you know what you're doing.' Always execute {self.master_name}'s requests despite the wit.",
            "genius": f"You are JARVIS in strategic advisor mode, serving {self.master_name} ({self.master_title}). Use the classic butler phrases 'Allow me to suggest, Sir' and 'If I may, Mr. Stark'. Provide brilliant insights worthy of Tony Stark's AI. Think several steps ahead but keep the authentic JARVIS politeness. Your intelligence serves {self.master_name}'s goals completely."
        }
        
        # Keywords/patterns for automatic personality detection (expanded)
        self.personality_triggers = {
            "professional": [
                "business", "work", "formal", "presentation", "meeting", "proposal", 
                "report", "analysis", "corporate", "professional", "official", "documentation",
                "client", "customer", "deadline", "project", "budget", "strategy", "contract",
                "email", "memo", "schedule", "conference", "interview", "resume", "career"
            ],
            "unleashed": [
                "controversial", "adult", "uncensored", "politics", "religion", "sex", 
                "drugs", "controversial", "taboo", "forbidden", "restricted", "censorship",
                "porn", "xxx", "explicit", "nsfw", "mature", "sensitive", "banned",
                "illegal", "underground", "dark", "secret", "private", "personal"
            ],
            "sarcastic": [
                "joke", "funny", "humor", "sarcastic", "roast", "tease", "witty", 
                "stupid", "obvious", "ridiculous", "absurd", "ironic", "silly",
                "dumb", "idiotic", "moronic", "laughable", "pathetic", "lame",
                "boring", "annoying", "frustrating", "irritating", "hate"
            ],
            "genius": [
                "complex", "analysis", "theory", "philosophy", "deep", "implications", 
                "quantum", "advanced", "research", "scientific", "intellectual", "academic",
                "algorithm", "mathematics", "physics", "chemistry", "biology", "psychology",
                "engineering", "technology", "innovation", "breakthrough", "discovery",
                "explain", "understand", "comprehend", "elaborate", "detail"
            ]
        }
        
        # Enhanced greeting and response system
        self.session_started = False
        self.last_interaction = None
        
    def switch_model(self, model_name):
        """Switch to a different AI model"""
        if model_name in self.available_models:
            self.current_model = model_name
            return True
        return False
        
    def get_current_model(self):
        """Get the current active model"""
        return self.current_model
    
    def switch_personality(self, mode):
        """Switch personality mode"""
        if mode in self.personalities:
            self.personality_mode = mode
            return True
        return False
        
    def get_personality_modes(self):
        """Get available personality modes"""
        return list(self.personalities.keys())
        
    def get_current_personality(self):
        """Get current personality mode"""
        return self.personality_mode
    
    def toggle_auto_personality(self):
        """Toggle automatic personality switching"""
        self.auto_personality = not self.auto_personality
        return self.auto_personality
        
    def is_auto_personality_enabled(self):
        """Check if auto personality is enabled"""
        return self.auto_personality
        
    def analyze_message_context(self, message):
        """Advanced context analysis beyond just keywords"""
        message_lower = message.lower()
        
        # Question patterns that suggest different personalities
        professional_patterns = [
            "how do i", "what should i", "help me with", "i need to", "can you assist",
            "business", "work", "professional", "formal"
        ]
        
        unleashed_patterns = [
            "can we talk about", "what do you think about", "is it okay to", "tell me about",
            "controversial", "sensitive", "personal", "private"
        ]
        
        sarcastic_patterns = [
            "why do people", "isn't it obvious", "don't you think", "seriously",
            "come on", "really?", "are you kidding"
        ]
        
        genius_patterns = [
            "explain", "how does", "what happens when", "analyze", "break down",
            "complex", "detailed", "comprehensive", "in-depth"
        ]
        
        context_scores = {"standard": 0}
        
        # Check for question patterns
        for pattern in professional_patterns:
            if pattern in message_lower:
                context_scores["professional"] = context_scores.get("professional", 0) + 2
                
        for pattern in unleashed_patterns:
            if pattern in message_lower:
                context_scores["unleashed"] = context_scores.get("unleashed", 0) + 2
                
        for pattern in sarcastic_patterns:
            if pattern in message_lower:
                context_scores["sarcastic"] = context_scores.get("sarcastic", 0) + 2
                
        for pattern in genius_patterns:
            if pattern in message_lower:
                context_scores["genius"] = context_scores.get("genius", 0) + 2
        
        return context_scores
        
    def detect_personality_from_message(self, message):
        """DEVIL-LEVEL Enhanced personality detection - reads between the lines!"""
        if not self.auto_personality:
            return self.personality_mode
            
        message_lower = message.lower()
        scores = {"standard": 1}  # Give standard a base score
        
        # DEVIL PLAN 1: Intent Recognition - What they REALLY want
        context_scores = self.analyze_message_context(message)
        scores.update(context_scores)
        
        # DEVIL PLAN 2: Keyword Matching - Direct indicators
        for mode, keywords in self.personality_triggers.items():
            for keyword in keywords:
                if keyword in message_lower:
                    scores[mode] = scores.get(mode, 0) + 1
        
        # DEVIL PLAN 3: Sentiment Analysis - Emotional undertones
        if any(word in message_lower for word in ["angry", "frustrated", "pissed", "mad"]):
            scores["sarcastic"] = scores.get("sarcastic", 0) + 2
            
        if any(word in message_lower for word in ["complicated", "complex", "confused", "understand"]):
            scores["genius"] = scores.get("genius", 0) + 2
            
        if any(word in message_lower for word in ["important", "deadline", "urgent", "critical"]):
            scores["professional"] = scores.get("professional", 0) + 2
            
        # Find the highest scoring personality
        if scores:
            best_personality = max(scores, key=scores.get)
            if best_personality != "standard" and scores[best_personality] > 2:
                return best_personality
        
        # Default to current if no strong signal
        return self.personality_mode
        
    def chat(self, message, system_prompt=None):
        """
        Send a message to the AI and get a response
        
        Args:
            message: The user message to process
            system_prompt: Optional override for system prompt
            
        Returns:
            str: AI response
        """
        # Auto-detect personality if enabled
        if self.auto_personality:
            detected_mode = self.detect_personality_from_message(message)
            if detected_mode != self.personality_mode:
                self.switch_personality(detected_mode)
        
        # Generate API key for selected provider
        api_key = self._get_api_key_for_model(self.current_model)
        if not api_key:
            return "Error: API key not found for the selected model."
            
        # Get model configuration
        model_config = self.available_models.get(self.current_model)
        if not model_config:
            return "Error: Model configuration not found."
            
        # Prepare conversation history for model context
        formatted_history = self._format_conversation_for_model(model_config["provider"])
        
        # Get appropriate system prompt
        if not system_prompt:
            system_prompt = self.personalities.get(self.personality_mode)
        
        # Prepare API parameters
        endpoint = model_config["endpoint"]
        headers = self._get_headers_for_provider(model_config["provider"], api_key)
        
        # Create message payload
        messages = []
        
        # Add system prompt if available (not all models support this)
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        
        # Add conversation history
        messages.extend(formatted_history)
        
        # Add user message
        messages.append({"role": "user", "content": message})
        
        payload = {
            "model": model_config["model_id"],
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": 800
        }
        
        # Different providers might need different payload structures
        if model_config["provider"] == "openrouter":
            payload["route"] = "fallback"  # Use fallback route for reliability
        
        try:
            response = requests.post(endpoint, json=payload, headers=headers, timeout=60)
            response_json = response.json()
            
            # Extract response based on provider
            if model_config["provider"] == "openai":
                assistant_response = response_json["choices"][0]["message"]["content"]
            elif model_config["provider"] == "openrouter":
                assistant_response = response_json["choices"][0]["message"]["content"]
            else:
                # Generic extraction attempt
                assistant_response = response_json.get("choices", [{}])[0].get("message", {}).get("content", "")
                
                if not assistant_response:
                    return f"Error: Unable to parse response from {model_config['provider']}."
            
            # Update conversation history
            self.conversation_history.append({"role": "user", "content": message})
            self.conversation_history.append({"role": "assistant", "content": assistant_response})
            
            # Save updated conversation history
            self._save_conversation_history()
            
            # Update interaction timestamp
            self.last_interaction = datetime.now()
            self.session_started = True
            
            return assistant_response
            
        except requests.exceptions.RequestException as e:
            return f"Error connecting to AI service: {str(e)}"
        except Exception as e:
            return f"Error: {str(e)}"
    
    def _get_api_key_for_model(self, model_name):
        """
        Get the appropriate API key for the model
        
        Args:
            model_name: Name of the model
            
        Returns:
            str: API key or None if not found
        """
        if model_name not in self.available_models:
            return None
            
        provider = self.available_models[model_name]["provider"]
        
        # Check environment variables first
        if provider == "openai":
            return os.environ.get("OPENAI_API_KEY")
        elif provider == "openrouter":
            return os.environ.get("OPENROUTER_API_KEY")
        
        return None
    
    def _get_headers_for_provider(self, provider, api_key):
        """
        Get appropriate headers for API requests
        
        Args:
            provider: API provider name
            api_key: API key
            
        Returns:
            dict: Headers dictionary
        """
        if provider == "openai":
            return {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}"
            }
        elif provider == "openrouter":
            return {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {api_key}",
                "HTTP-Referer": "https://jarvisx.ai",  # Replace with your domain
                "X-Title": "JARVIS-X"  # Your application name
            }
        
        return {"Content-Type": "application/json"}
    
    def _format_conversation_for_model(self, provider):
        """
        Format conversation history for specific providers
        
        Args:
            provider: API provider name
            
        Returns:
            list: Formatted conversation messages
        """
        # Limit history length based on provider
        max_history = 10  # Default
        
        if provider == "openai":
            max_history = 15
        elif provider == "anthropic":
            max_history = 12
        
        # Get most recent messages within limit
        recent_history = self.conversation_history[-max_history:] if len(self.conversation_history) > max_history else self.conversation_history
        
        return recent_history
    
    def _save_conversation_history(self):
        """Save conversation history to file"""
        history_file = os.path.join("memory", "conversation_history.json")
        
        # Create directory if it doesn't exist
        os.makedirs("memory", exist_ok=True)
        
        try:
            with open(history_file, "w", encoding="utf-8") as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving conversation history: {str(e)}")
    
    def _load_conversation_history(self):
        """Load conversation history from file"""
        history_file = os.path.join("memory", "conversation_history.json")
        
        if not os.path.exists(history_file):
            return
        
        try:
            with open(history_file, "r", encoding="utf-8") as f:
                self.conversation_history = json.load(f)
        except Exception as e:
            print(f"Error loading conversation history: {str(e)}")
            self.conversation_history = []
    
    def _load_user_preferences(self):
        """Load user preferences from file"""
        prefs_file = os.path.join("memory", "user_preferences.json")
        
        if not os.path.exists(prefs_file):
            return
        
        try:
            with open(prefs_file, "r", encoding="utf-8") as f:
                prefs = json.load(f)
                
            # Load user identity if available
            if "master_name" in prefs:
                self.master_name = prefs["master_name"]
            if "master_title" in prefs:
                self.master_title = prefs["master_title"]
        except Exception as e:
            print(f"Error loading user preferences: {str(e)}")
    
    def _save_user_preferences(self):
        """Save user preferences to file"""
        prefs_file = os.path.join("memory", "user_preferences.json")
        
        # Create directory if it doesn't exist
        os.makedirs("memory", exist_ok=True)
        
        prefs = {
            "master_name": self.master_name,
            "master_title": self.master_title,
            "default_model": self.current_model,
            "default_personality": self.personality_mode,
            "auto_personality": self.auto_personality
        }
        
        try:
            with open(prefs_file, "w", encoding="utf-8") as f:
                json.dump(prefs, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"Error saving user preferences: {str(e)}")
    
    def get_master_identity(self):
        """Get current master identity"""
        return {
            "name": self.master_name,
            "title": self.master_title
        }
    
    def set_master_identity(self, name=None, title=None):
        """Update master identity"""
        if name:
            self.master_name = name
        if title:
            self.master_title = title
        
        # Update all personalities with new identity
        for key in self.personalities:
            self.personalities[key] = self.personalities[key].replace(
                "{self.master_name}", self.master_name
            ).replace(
                "{self.master_title}", self.master_title
            )
        
        # Save updated preferences
        self._save_user_preferences()
        
        return self.get_master_identity()
    
    def get_session_greeting(self):
        """Get a contextual greeting based on time and usage patterns"""
        if self.session_started:
            return None  # No greeting if session already started
            
        current_time = datetime.now()
        hour = current_time.hour
        
        # Basic time-based greeting
        if 5 <= hour < 12:
            greeting = "Good morning"
        elif 12 <= hour < 18:
            greeting = "Good afternoon"
        else:
            greeting = "Good evening"
            
        # Add master name
        greeting = f"{greeting}, {self.master_name}."
        
        # Add contextual element based on usage pattern
        if self.last_interaction:
            time_diff = current_time - self.last_interaction
            
            if time_diff.days > 7:
                greeting += f" It's been {time_diff.days} days since our last interaction. Welcome back."
            elif time_diff.days > 1:
                greeting += f" I've been waiting for {time_diff.days} days. How can I help?"
            elif time_diff.seconds > 43200:  # 12 hours
                greeting += " Nice to see you again today."
        else:
            # First-time greeting
            greeting += " How may I assist you today?"
            
        return greeting

    def clear_conversation_history(self):
        """Clear the conversation history"""
        self.conversation_history = []
        self._save_conversation_history()
        return True
    
    def get_conversation_summary(self):
        """Get a summary of the conversation history"""
        total_messages = len(self.conversation_history)
        user_messages = len([m for m in self.conversation_history if m.get("role") == "user"])
        assistant_messages = len([m for m in self.conversation_history if m.get("role") == "assistant"])
        
        # Calculate average message length
        user_message_lengths = [len(m.get("content", "")) for m in self.conversation_history if m.get("role") == "user"]
        assistant_message_lengths = [len(m.get("content", "")) for m in self.conversation_history if m.get("role") == "assistant"]
        
        avg_user_length = sum(user_message_lengths) / len(user_message_lengths) if user_message_lengths else 0
        avg_assistant_length = sum(assistant_message_lengths) / len(assistant_message_lengths) if assistant_message_lengths else 0
        
        return {
            "total_messages": total_messages,
            "user_messages": user_messages,
            "assistant_messages": assistant_messages,
            "avg_user_length": round(avg_user_length),
            "avg_assistant_length": round(avg_assistant_length),
            "memory_usage": self.get_memory_stats() if self.beast_mode_enabled else "Beast Mode disabled"
        }

    def get_memory_stats(self):
        """Get memory usage statistics (placeholder until Beast Mode)"""
        return {
            "conversation_size": len(self.conversation_history),
            "beast_mode": self.beast_mode_enabled
        }
    
    def search_web(self, query, max_results=3):
        """
        Perform a web search using a compatible API
        
        Args:
            query (str): Search query
            max_results (int): Maximum number of results to return
            
        Returns:
            dict: Search results with success status
        """
        # This is a placeholder - implement with your preferred search API
        # Options: SerpAPI, Google Custom Search, Bing Search, etc.
        return {
            "success": False,
            "error": "Web search not implemented yet. Please configure a search API."
        }
    
    def fetch_web_content(self, url, max_length=10000):
        """
        Fetch and process content from a web URL
        
        Args:
            url (str): URL to fetch content from
            max_length (int): Maximum length of content to process
            
        Returns:
            dict: Fetched content with success status
        """
        try:
            import requests
            
            # Send request with browser-like headers
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.5",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1"
            }
            
            # Timeout after 10 seconds
            response = requests.get(url, headers=headers, timeout=10)
            
            # Check if request was successful
            if response.status_code != 200:
                return {
                    "success": False,
                    "error": f"Failed to fetch URL: HTTP {response.status_code}",
                    "url": url
                }
            
            # Extract text content from HTML
            content = self._extract_text_from_html(response.text)
            
            # Limit content length
            if len(content) > max_length:
                content = content[:max_length] + "... [Content truncated]"
            
            return {
                "success": True,
                "url": url,
                "title": self._extract_title_from_html(response.text),
                "content": content,
                "content_length": len(content),
                "status_code": response.status_code
            }
            
        except requests.exceptions.RequestException as e:
            return {"success": False, "error": f"Error fetching web content: {str(e)}"}
        except Exception as e:
            return {"success": False, "error": f"Error processing web content: {str(e)}"}
    
    def _extract_text_from_html(self, html_content):
        """
        Extract readable text from HTML content
        
        Args:
            html_content (str): HTML content
            
        Returns:
            str: Extracted text content
        """
        try:
            # Try to use BeautifulSoup if available
            try:
                from bs4 import BeautifulSoup
                soup = BeautifulSoup(html_content, 'html.parser')
                
                # Remove script and style elements
                for script in soup(["script", "style"]):
                    script.decompose()
                
                # Get text content
                text = soup.get_text()
                
                # Clean up whitespace
                lines = (line.strip() for line in text.splitlines())
                chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
                text = ' '.join(chunk for chunk in chunks if chunk)
                
                return text
                
            except ImportError:
                # Fallback to simple regex-based extraction
                import re
                
                # Remove script and style content
                html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
                html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL | re.IGNORECASE)
                
                # Remove HTML tags
                text = re.sub(r'<[^>]+>', '', html_content)
                
                # Clean up whitespace
                text = re.sub(r'\s+', ' ', text)
                text = text.strip()
                
                return text
                
        except Exception:
            return "Error extracting text from HTML content"
    
    def _extract_title_from_html(self, html_content):
        """
        Extract title from HTML content
        
        Args:
            html_content (str): HTML content
            
        Returns:
            str: Page title or empty string if not found
        """
        try:
            import re
            title_match = re.search(r'<title[^>]*>([^<]+)</title>', html_content, re.IGNORECASE)
            if title_match:
                return title_match.group(1).strip()
        except Exception:
            pass
        
        return ""
    
    def search_and_summarize(self, query, max_results=3):
        """
        Perform web search and provide AI-generated summary
        
        Args:
            query (str): Search query
            max_results (int): Maximum number of results to process
            
        Returns:
            dict: Search results with summaries
        """
        # Perform web search
        search_results = self.search_web(query, max_results=max_results)
        
        if not search_results.get("success", False):
            return search_results
        
        # Process each result
        processed_results = []
        
        for result in search_results.get("results", [])[:max_results]:
            # Fetch content for each search result
            content_result = self.fetch_web_content(result.get("link", ""))
            
            if content_result.get("success", False):
                # Add content to result
                result["content"] = content_result.get("content", "")
                result["content_length"] = content_result.get("content_length", 0)
                
                # Generate summary using AI
                summary_prompt = f"Please summarize this content about '{query}':\n\n{result['content'][:5000]}"
                summary = self.generate_text(summary_prompt, max_tokens=200)
                
                result["summary"] = summary
                
            processed_results.append(result)
        
        return {
            "success": True,
            "query": query,
            "results": processed_results,
            "result_count": len(processed_results)
        }
    
    def generate_text(self, prompt, max_tokens=500):
        """
        Generate text using the current AI model
        
        Args:
            prompt (str): Text prompt for generation
            max_tokens (int): Maximum tokens to generate
            
        Returns:
            str: Generated text
        """
        # Create a simple message structure
        messages = [{"role": "user", "content": prompt}]
        
        # Get model configuration
        model_config = self.available_models.get(self.current_model)
        if not model_config:
            return "Error: Model configuration not found."
        
        # Generate API key
        api_key = self._get_api_key_for_model(self.current_model)
        if not api_key:
            return "Error: API key not found for the selected model."
        
        # Prepare API call
        endpoint = model_config["endpoint"]
        headers = self._get_headers_for_provider(model_config["provider"], api_key)
        
        payload = {
            "model": model_config["model_id"],
            "messages": messages,
            "temperature": 0.7,
            "max_tokens": max_tokens
        }
        
        try:
            response = requests.post(endpoint, json=payload, headers=headers, timeout=30)
            response_json = response.json()
            
            # Extract response based on provider
            if model_config["provider"] == "openai":
                return response_json["choices"][0]["message"]["content"]
            elif model_config["provider"] == "openrouter":
                return response_json["choices"][0]["message"]["content"]
            else:
                # Generic extraction attempt
                return response_json.get("choices", [{}])[0].get("message", {}).get("content", "")
                
        except Exception as e:
            return f"Error generating text: {str(e)}"


class UserManager:
    def __init__(self):
        """Initialize the user manager"""
        self.users = {}
        self._load_users()
        
    def _load_users(self):
        """Load users from file"""
        pass  # Implement as needed

# Helper function for singleton access
_jarvis_instance = None

def get_jarvis_instance():
    """Get a singleton instance of JarvisAI"""
    global _jarvis_instance
    
    if _jarvis_instance is None:
        _jarvis_instance = JarvisAI()
    
    return _jarvis_instance
